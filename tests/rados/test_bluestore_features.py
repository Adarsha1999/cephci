"""
This file contains the  methods to verify the Bluestore features.
"""

import datetime
import time

from ceph.ceph_admin import CephAdmin
from ceph.rados.core_workflows import RadosOrchestrator
from tests.rados.monitor_configurations import MonConfigMethods
from utility.log import Log
from utility.utils import method_should_succeed

log = Log(__name__)


def run(ceph_cluster, **kw):
    """
    Performs various pool related validation tests
    Returns:
        1 -> Fail, 0 -> Pass
    """
    log.info(run.__doc__)
    config = kw["config"]
    cephadm = CephAdmin(cluster=ceph_cluster, **config)
    rados_object = RadosOrchestrator(node=cephadm)
    mon_obj = MonConfigMethods(rados_obj=rados_object)
    ceph_nodes = kw.get("ceph_nodes")
    osd_list = []
    total_osd_app_mem = {}

    for node in ceph_nodes:
        if node.role == "osd":
            node_osds = rados_object.collect_osd_daemon_ids(node)
            osd_list = osd_list + node_osds

    target_configs = config["cache_trim_max_skip_pinned"]["configurations"]
    max_skip_pinned_value = int(
        mon_obj.get_config(section="osd", param="bluestore_cache_trim_max_skip_pinned")
    )

    # Check the default value of the  bluestore_cache_trim_max_skip_pinned value
    if max_skip_pinned_value != 1000:
        log.error(
            "The default value of  bluestore_cache_trim_max_skip_pinned not equal to 1000"
        )
        raise Exception(
            "The default value of bluestore_cache_trim_max_skip_pinned is not 1000"
        )

    # Creating pools and starting the test
    for entry in target_configs.values():
        log.debug(
            f"Creating {entry['pool_type']} pool on the cluster with name {entry['pool_name']}"
        )
        if entry.get("pool_type", "replicated") == "erasure":
            method_should_succeed(
                rados_object.create_erasure_pool, name=entry["pool_name"], **entry
            )
        else:
            method_should_succeed(
                rados_object.create_pool,
                **entry,
            )

        if not rados_object.bench_write(**entry):
            log.error("Failed to write objects into the EC Pool")
            return 1
        rados_object.bench_read(**entry)
    log.info("Finished writing data into the pool")

    # performing scrub and deep-scrub
    rados_object.run_scrub()
    rados_object.run_deep_scrub()
    time.sleep(10)

    rados_object.change_heap_profiler_state(osd_list, "start")
    # Executing tests for 45 minutes
    time_execution = datetime.datetime.now() + datetime.timedelta(minutes=45)
    while datetime.datetime.now() < time_execution:
        # Get all OSDs heap dump
        heap_dump = rados_object.get_heap_dump(osd_list)
        # get the osd application used memory
        osd_app_mem = get_bytes_used_by_app(heap_dump)
        total_osd_app_mem = mergeDictionary(total_osd_app_mem, osd_app_mem)
        # wait for 10 seconds and collecting the memory
        time.sleep(10)
    for osd_id, mem_list in total_osd_app_mem.items():
        mem_growth = is_what_percent_mem(mem_list)
        if mem_growth > 80:
            log.error(
                f"The osd.{osd_id} consuming more memory with the relative memory growth {mem_growth}"
            )
            raise Exception("No warning generated by PG Autoscaler")
        log.info(f"The relative memory growth for the osd.{osd_id} is {mem_growth} ")

    rados_object.change_heap_profiler_state(osd_list, "stop")

    # check fo the crashes in the cluster
    crash_list = rados_object.do_crash_ls()
    if not crash_list:
        return 0
    else:
        return 1


def is_what_percent_mem(mem_list):
    """
    To calculate the relative memory growth
    Args:
         mem_list: list of mllac values of the application
    Return:
          result : Finds the relative growth of min amd max values

    """
    min_mem = int(min(mem_list))
    max_mem = int(max(mem_list))
    result = float(((max_mem - min_mem) * 100) / min_mem)
    return result


def get_bytes_used_by_app(osds_heap_dump):
    """
    To gets the bytes ued by the application in the heap dump
    Args:
         osds_heap_dump: List of osd heap dumps
         Example of heap dump:
              MALLOC:      429538736 (  409.6 MiB) Bytes in use by application
              MALLOC: +            0 (    0.0 MiB) Bytes in page heap freelist
              MALLOC: +      8934864 (    8.5 MiB) Bytes in central cache freelist
              MALLOC: +      3876864 (    3.7 MiB) Bytes in transfer cache freelist
              MALLOC: +     20636800 (   19.7 MiB) Bytes in thread cache freelists

    Return:
          osd_id_app_mem: Returns a dictionary with OSD id as a key and
                          malloc for the OSD application
                   Example: {1:['429538736' , '456789',..], 2:['21312','76576',..]...}
    """
    osd_id_app_mem = {}
    app_str = "Bytes in use by application"
    for osd_id, osd_dump in osds_heap_dump.items():
        malloc_str = osd_dump.split("MALLOC:")
        result = [app_dump for app_dump in malloc_str if app_str in app_dump]
        result = (" ".join(result)).strip()
        result_list = result.split("(")
        osd_id_app_mem[osd_id] = result_list[0].strip()
    return osd_id_app_mem


# Merge dictionaries
def mergeDictionary(dict_1, dict_2):
    """
    Merge two dictionaries
    Args:
        dict_1 & dict_2 : are two dictionaries
    Return:
        dict_1 : Returns a dictionary after merging two dictionaries.
    """
    for key, value in dict_2.items():
        if key in dict_1:
            if isinstance(dict_1[key], list):
                dict_1[key].append(value)
            else:
                temp_list = [dict_1[key]]
                temp_list.append(value)
                dict_1[key] = temp_list
        else:
            dict_1[key] = value
    return dict_1
